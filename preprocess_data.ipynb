{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import to_timestamp, col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "your 131072x1 screen size is bogus. expect trouble\n",
      "23/09/12 00:01:37 WARN Utils: Your hostname, SUSHAN resolves to a loopback address: 127.0.1.1; using 172.22.228.148 instead (on interface eth0)\n",
      "23/09/12 00:01:37 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/09/12 00:01:39 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName('finalproject')\\\n",
    "        .config('spark.driver.extraClassPath','/usr/lib/jvm/java-17-openjdk-amd64/lib/postgresql-42.6.0.jar')\\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "stationInfo_df = spark.read.csv(\"data/Fuel_Station_Information.csv\", header=True, inferSchema=True)\n",
    "hourlyPrices_df = spark.read.csv(\"data/Hourly_Gasoline_Prices.csv\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Id: integer (nullable = true)\n",
      " |-- isSelf: integer (nullable = true)\n",
      " |-- Price: double (nullable = true)\n",
      " |-- Date: timestamp (nullable = true)\n",
      "\n",
      "root\n",
      " |-- Id: string (nullable = true)\n",
      " |-- Fuel_station_manager: string (nullable = true)\n",
      " |-- Petrol_company: string (nullable = true)\n",
      " |-- Type: string (nullable = true)\n",
      " |-- Station_name: string (nullable = true)\n",
      " |-- City: string (nullable = true)\n",
      " |-- Latitude: string (nullable = true)\n",
      " |-- Longitudine: string (nullable = true)\n",
      "\n",
      "None None\n"
     ]
    }
   ],
   "source": [
    "print(hourlyPrices_df.printSchema(), stationInfo_df.printSchema())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Duplicates\n",
    "hourlyPrices_df = hourlyPrices_df.dropDuplicates()\n",
    "stationInfo_df = stationInfo_df.dropDuplicates()\n",
    "\n",
    "# Drop rows with any null values\n",
    "hourlyPrices_df = hourlyPrices_df.dropna()\n",
    "stationInfo_df = stationInfo_df.dropna()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "hourlyPrices_df = hourlyPrices_df.withColumn(\"Date\", to_timestamp(col(\"Date\"), \"yyyy-MM-dd HH:mm:ss\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of records in hourlyPrices: 2444337\n",
      "Total number of records in stationInfo: 22129\n"
     ]
    }
   ],
   "source": [
    "countt = hourlyPrices_df.count()\n",
    "print(\"Total number of records in hourlyPrices:\", countt)\n",
    "counttt = stationInfo_df.count()\n",
    "print(\"Total number of records in stationInfo:\", counttt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/12 00:05:14 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "hourlyPrices_df.write.parquet(\"data/cleaned_fuel_prices.parquet\", mode= \"overwrite\", compression= \"snappy\")\n",
    "stationInfo_df.write.parquet(\"data/cleaned_station_info.parquet\", mode= \"overwrite\", compression= \"snappy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the Parquet files\n",
    "hourlyPrices_df = spark.read.parquet(\"data/cleaned_fuel_prices.parquet\")\n",
    "stationInfo_df = spark.read.parquet(\"data/cleaned_station_info.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import FloatType\n",
    "\n",
    "stationInfo_df = stationInfo_df.withColumn(\"Latitude\", stationInfo_df[\"Latitude\"].cast(FloatType()))\n",
    "stationInfo_df = stationInfo_df.withColumn(\"Longitudine\", stationInfo_df[\"Longitudine\"].cast(FloatType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Id: integer (nullable = true)\n",
      " |-- isSelf: integer (nullable = true)\n",
      " |-- Price: double (nullable = true)\n",
      " |-- Date: timestamp (nullable = true)\n",
      "\n",
      "root\n",
      " |-- Id: string (nullable = true)\n",
      " |-- Fuel_station_manager: string (nullable = true)\n",
      " |-- Petrol_company: string (nullable = true)\n",
      " |-- Type: string (nullable = true)\n",
      " |-- Station_name: string (nullable = true)\n",
      " |-- City: string (nullable = true)\n",
      " |-- Latitude: float (nullable = true)\n",
      " |-- Longitudine: float (nullable = true)\n",
      "\n",
      "None None\n"
     ]
    }
   ],
   "source": [
    "print(hourlyPrices_df.printSchema(), stationInfo_df.printSchema())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the JDBC connection properties\n",
    "jdbc_url = \"jdbc:postgresql://localhost:5432/sparkProject\"\n",
    "jdbc_properties = {\n",
    "    \"user\": \"sushan\",\n",
    "    \"password\": \"7446\",\n",
    "    \"driver\": \"org.postgresql.Driver\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "\n",
    "# Save DataFrames to PostgreSQL tables\n",
    "hourlyPrices_df.write.jdbc(url=jdbc_url, table=\"hourly_gasoline_prices\", mode=\"overwrite\", properties=jdbc_properties)\n",
    "stationInfo_df.write.jdbc(url=jdbc_url, table=\"fuel_station_information\", mode=\"overwrite\", properties=jdbc_properties)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop the SparkSession\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myVenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
