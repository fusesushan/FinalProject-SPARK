{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import to_timestamp, col, lag, avg, abs, lit, min, udf, dense_rank\n",
    "from pyspark.sql.window import Window\n",
    "from haversine import haversine, Unit\n",
    "from pyspark.sql.types import FloatType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "your 131072x1 screen size is bogus. expect trouble\n",
      "23/09/13 17:26:36 WARN Utils: Your hostname, SUSHAN resolves to a loopback address: 127.0.1.1; using 172.22.230.162 instead (on interface eth0)\n",
      "23/09/13 17:26:36 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/09/13 17:26:38 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName('finalproject')\\\n",
    "        .config('spark.driver.extraClassPath','/usr/lib/jvm/java-17-openjdk-amd64/lib/postgresql-42.6.0.jar')\\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/13 17:27:16 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "stationInfo_df = spark.read.csv(\"data/Fuel_Station_Information.csv\", header=True, inferSchema=True)\n",
    "hourlyPrices_df = spark.read.csv(\"data/Hourly_Gasoline_Prices.csv\", header=True, inferSchema=True)\n",
    "hourlyPrices_df = hourlyPrices_df.dropDuplicates()\n",
    "stationInfo_df = stationInfo_df.dropDuplicates()\n",
    "hourlyPrices_df = hourlyPrices_df.dropna()\n",
    "stationInfo_df = stationInfo_df.dropna()\n",
    "hourlyPrices_df = hourlyPrices_df.withColumn(\"Date\", to_timestamp(col(\"Date\"), \"yyyy-MM-dd HH:mm:ss\"))\n",
    "hourlyPrices_df.write.parquet(\"data/cleaned_fuel_prices.parquet\", mode= \"overwrite\", compression= \"snappy\")\n",
    "stationInfo_df.write.parquet(\"data/cleaned_station_info.parquet\", mode= \"overwrite\", compression= \"snappy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "yaml_file_path = 'credentials.yaml'\n",
    "\n",
    "with open(yaml_file_path, 'r') as yaml_file:\n",
    "    config = yaml.safe_load(yaml_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Read the Parquet files\n",
    "hourlyPrices_df = spark.read.parquet(\"data/cleaned_fuel_prices.parquet\")\n",
    "stationInfo_df = spark.read.parquet(\"data/cleaned_station_info.parquet\")\n",
    "stationInfo_df = stationInfo_df.withColumn(\"Latitude\", stationInfo_df[\"Latitude\"].cast(FloatType()))\n",
    "stationInfo_df = stationInfo_df.withColumn(\"Longitudine\", stationInfo_df[\"Longitudine\"].cast(FloatType()))\n",
    "# Define the JDBC connection properties\n",
    "jdbc_url = \"jdbc:postgresql://localhost:5432/sparkProject\"\n",
    "jdbc_properties = {\n",
    "    \"user\": config['postgres'][\"user\"],\n",
    "    \"password\": str(config['postgres'][\"password\"]),\n",
    "    \"driver\": \"org.postgresql.Driver\"\n",
    "}\n",
    "# Save DataFrames to PostgreSQL tables\n",
    "hourlyPrices_df.write.jdbc(url=jdbc_url, table=\"hourly_gasoline_prices\", mode=\"overwrite\", properties=jdbc_properties)\n",
    "stationInfo_df.write.jdbc(url=jdbc_url, table=\"fuel_station_information\", mode=\"overwrite\", properties=jdbc_properties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read DataFrames to PostgreSQL tables\n",
    "hourlyPrices_df = spark.read.format('jdbc').options(url=\"jdbc:postgresql://localhost:5432/sparkProject\", driver = 'org.postgresql.Driver', dbtable='hourly_gasoline_prices', user=config['postgres'][\"user\"],password=str(config['postgres'][\"password\"])).load()\n",
    "stationInfo_df = spark.read.format('jdbc').options(url=\"jdbc:postgresql://localhost:5432/sparkProject\", driver = 'org.postgresql.Driver', dbtable='fuel_station_information', user=config['postgres'][\"user\"],password=str(config['postgres'][\"password\"])).load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1. Find the `Most Stable Company` and `Most Volatile Company` based on `Average Daily Price Variation`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 31:=================================================>        (6 + 1) / 7]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Stable Company:\n",
      "Company: Blanco petroli\n",
      "Average Daily Price Variation: 0.008190127970749542\n",
      "_*_*_*_*_*_*_*_*_*_*_*_*_*_*_*_*_*_*_*_*_*_*_*_*_*_*_*_*_*_*_*_*_*\n",
      "Most Volatile Company:\n",
      "Company: COLAGROSSI CARBURANTI\n",
      "Average Daily Price Variation: 0.2734666666666667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_forAvg = hourlyPrices_df.join(stationInfo_df.select(\"Id\", \"Petrol_company\"), \"Id\")\n",
    "\n",
    "window_spec = Window.partitionBy(\"Petrol_company\").orderBy(\"Date\")\n",
    "\n",
    "daily_var = df_forAvg.withColumn(\"Previous_Price\", lag(\"Price\").over(window_spec))\n",
    "daily_var = daily_var.withColumn(\"Price_Variation\", abs(col(\"Price\") - col(\"Previous_Price\")))\n",
    "daily_var = daily_var.filter(col(\"Previous_Price\").isNotNull())\n",
    "avg_variation_df = daily_var.groupBy(\"Petrol_company\").agg(avg(\"Price_Variation\").alias(\"Avg_Daily_Variation\"))\n",
    "\n",
    "min_variation_company = avg_variation_df.orderBy(\"Avg_Daily_Variation\").first()\n",
    "max_variation_company = avg_variation_df.orderBy(avg_variation_df[\"Avg_Daily_Variation\"].desc()).first()\n",
    "\n",
    "print(\"Most Stable Company:\")\n",
    "print(\"Company:\", min_variation_company[\"Petrol_company\"])\n",
    "print(\"Average Daily Price Variation:\", min_variation_company[\"Avg_Daily_Variation\"])\n",
    "\n",
    "print(\"_*\"*33)\n",
    "\n",
    "print(\"Most Volatile Company:\")\n",
    "print(\"Company:\", max_variation_company[\"Petrol_company\"])\n",
    "print(\"Average Daily Price Variation:\", max_variation_company[\"Avg_Daily_Variation\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "data_t1 = [\n",
    "    (\"Most Stable Company\", min_variation_company[\"Petrol_company\"], min_variation_company[\"Avg_Daily_Variation\"]),\n",
    "    (\"Most Volatile Company\", max_variation_company[\"Petrol_company\"],  max_variation_company[\"Avg_Daily_Variation\"])\n",
    "]\n",
    "\n",
    "schema_t1 = [\"Category\", \"Company\", \"Average_Daily_Price_Variation\"]\n",
    "df_t1 = spark.createDataFrame(data_t1, schema_t1)\n",
    "\n",
    "jdbc_url = \"jdbc:postgresql://localhost:5432/sparkProject\"\n",
    "jdbc_properties = {\n",
    "    \"user\": config['postgres'][\"user\"],\n",
    "    \"password\": str(config['postgres'][\"password\"]),\n",
    "    \"driver\": \"org.postgresql.Driver\"\n",
    "}\n",
    "df_t1.write.jdbc(url=jdbc_url, table=\"stable_volatile_company\", mode=\"overwrite\", properties=jdbc_properties)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2. `Find 9 nearest stations to a certain reference station and calculate the price difference between the fuel station and its nearest competitor`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_latitude = 40.7160385\n",
    "ref_longitude = 14.9413282"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/13 17:29:14 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/09/13 17:29:14 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/09/13 17:29:14 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/09/13 17:29:14 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/09/13 17:29:14 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/09/13 17:29:17 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/09/13 17:29:17 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/09/13 17:29:17 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/09/13 17:29:17 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/09/13 17:29:19 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/09/13 17:29:19 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/09/13 17:29:19 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/09/13 17:29:19 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/09/13 17:29:22 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/09/13 17:29:22 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/09/13 17:29:22 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/09/13 17:29:22 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/09/13 17:29:41 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/09/13 17:29:41 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/09/13 17:29:41 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/09/13 17:29:41 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Stage 41:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+-----+------------+---------------------------+------------------------------------+\n",
      "|   Id|isSelf|Price|min_distance|Price_of_Nearest_Competitor|PriceDifferenceFromNearestCompetitor|\n",
      "+-----+------+-----+------------+---------------------------+------------------------------------+\n",
      "|38792|     1| 1.82|  0.22640005|                       null|                                null|\n",
      "|52830|     1|1.929|   0.3241125|                       1.82|                 0.10899999999999999|\n",
      "|17152|     0|2.014|    2.730627|                      1.929|                 0.08499999999999974|\n",
      "|17409|     0|1.979|    2.848063|                      2.014|                 -0.0349999999999997|\n",
      "|51180|     1|1.759|   2.9082253|                      1.979|                 -0.2200000000000002|\n",
      "|27109|     1|1.709|   4.2238717|                      1.759|                -0.04999999999999982|\n",
      "|46547|     1|1.799|    4.730566|                      1.709|                 0.08999999999999986|\n",
      "|23047|     1|1.767|   4.7989206|                      1.799|                -0.03200000000000003|\n",
      "|42180|     0|1.789|    5.923098|                      1.767|                 0.02200000000000002|\n",
      "+-----+------+-----+------------+---------------------------+------------------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "@udf(FloatType())\n",
    "def calculate_distance(lat1, lon1, lat2, lon2):\n",
    "    return haversine((lat1, lon1), (lat2, lon2), unit=Unit.KILOMETERS)\n",
    "\n",
    "stationInfo_df = stationInfo_df.filter((col(\"Latitude\").isNotNull()) & (col(\"Longitudine\").isNotNull()))\n",
    "dis_stationInfo_df = stationInfo_df.withColumn(\"distance\",calculate_distance(lit(ref_latitude),\n",
    "                     lit(ref_longitude),col(\"Latitude\"),col(\"Longitudine\")))\n",
    "min_distance_df = dis_stationInfo_df.groupBy(\"Id\").agg(min(\"distance\").alias(\"min_distance\"))\n",
    "price_diff_df = hourlyPrices_df.join(min_distance_df, \"Id\", \"inner\")\n",
    "\n",
    "window_spec = Window.orderBy(col(\"min_distance\"))\n",
    "ranked_stations_df = price_diff_df.withColumn(\"rank\", dense_rank().over(window_spec))\n",
    "deduplicated_stations_df = ranked_stations_df.dropDuplicates(['Id']).drop(\"Date\")\n",
    "filtered_stations_df = deduplicated_stations_df.filter(col(\"rank\") <= 9).orderBy(\"rank\")\n",
    "filtered_stations_df = filtered_stations_df.drop('rank')\n",
    "\n",
    "window_spec = Window.orderBy(\"min_distance\")\n",
    "filtered_stations_df = filtered_stations_df.withColumn(\"Price_of_Nearest_Competitor\", lag(\"Price\").over(window_spec))\n",
    "filtered_stations_df = filtered_stations_df.withColumn(\n",
    "    \"PriceDifferenceFromNearestCompetitor\",\n",
    "    col(\"Price\") - col(\"Price_of_Nearest_Competitor\")\n",
    ")\n",
    "filtered_stations_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/13 17:29:51 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/09/13 17:29:51 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/09/13 17:29:51 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/09/13 17:29:51 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/09/13 17:29:51 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/09/13 17:29:52 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/09/13 17:29:52 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/09/13 17:29:52 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/09/13 17:29:52 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/09/13 17:29:52 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/09/13 17:29:52 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/09/13 17:29:52 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/09/13 17:29:52 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/09/13 17:29:58 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/09/13 17:29:58 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/09/13 17:29:58 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/09/13 17:29:58 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/09/13 17:29:59 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/09/13 17:29:59 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/09/13 17:29:59 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/09/13 17:29:59 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "jdbc_url = \"jdbc:postgresql://localhost:5432/sparkProject\"\n",
    "jdbc_properties = {\n",
    "    \"user\": config['postgres'][\"user\"],\n",
    "    \"password\": str(config['postgres'][\"password\"]),\n",
    "    \"driver\": \"org.postgresql.Driver\"\n",
    "}\n",
    "# Save DataFrames to PostgreSQL tables\n",
    "filtered_stations_df.write.jdbc(url=jdbc_url, table=\"Nearest_competitor_comparision\", mode=\"overwrite\", properties=jdbc_properties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop the SparkSession\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "theVenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
